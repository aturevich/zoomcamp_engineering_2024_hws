{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\tools\\java\n",
      "C:\\tools\\spark\\spark-3.3.2-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"11.0.21\" 2023-10-17 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.21+9-LTS-193)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.21+9-LTS-193, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: py4j\n",
      "Version: 0.10.9.5\n",
      "Summary: Enables Python programs to dynamically access arbitrary Java objects\n",
      "Home-page: https://www.py4j.org/\n",
      "Author: Barthelemy Dagenais\n",
      "Author-email: barthelemy@infobart.com\n",
      "License: BSD License\n",
      "Location: c:\\users\\artyom\\anaconda3\\envs\\spark_env\\lib\\site-packages\n",
      "Requires: \n",
      "Required-by: pyspark\n"
     ]
    }
   ],
   "source": [
    "!pip show py4j               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.3.2\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: c:\\users\\artyom\\anaconda3\\envs\\spark_env\\lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('../data/fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.read_csv('../data/fhv_tripdata_2019-10.csv', nrows = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),  \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('../data/fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('fhv/2019/10/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|pickup_date|dropoff_date|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+\n",
      "|              B00271|2019-10-02 00:42:46|2019-10-02 01:25:23|         264|         265|   null|                B00271| 2019-10-02|  2019-10-02|\n",
      "|              B03107|2019-10-02 17:57:32|2019-10-02 19:22:26|          24|         256|   null|                B03107| 2019-10-02|  2019-10-02|\n",
      "|              B01087|2019-10-01 09:01:20|2019-10-01 09:19:33|         143|         170|   null|                B01087| 2019-10-01|  2019-10-01|\n",
      "|              B02550|2019-10-01 11:21:37|2019-10-01 11:30:56|         264|         167|   null|                B02550| 2019-10-01|  2019-10-01|\n",
      "|              B01721|2019-10-01 00:50:10|2019-10-01 00:55:37|         264|         256|   null|                B01721| 2019-10-01|  2019-10-01|\n",
      "|              B00310|2019-10-02 20:29:21|2019-10-02 20:38:08|         264|          31|   null|                B02887| 2019-10-02|  2019-10-02|\n",
      "|              B01536|2019-10-01 11:30:07|2019-10-01 11:41:28|         264|         200|   null|                B01536| 2019-10-01|  2019-10-01|\n",
      "|              B00756|2019-10-02 06:19:00|2019-10-02 06:52:00|         264|         264|   null|                B00756| 2019-10-02|  2019-10-02|\n",
      "|              B00536|2019-10-02 11:10:54|2019-10-02 11:20:35|         264|          89|   null|                B00536| 2019-10-02|  2019-10-02|\n",
      "|              B00256|2019-10-01 13:59:37|2019-10-01 14:04:24|         264|         264|   null|                B00256| 2019-10-01|  2019-10-01|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date('pickup_datetime')) \\\n",
    "    .withColumn('dropoff_date', F.to_date('dropoff_datetime')) \\\n",
    "    .show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date('pickup_datetime')) \\\n",
    "    .filter(F.col(\"pickup_date\") == \"2019-10-15\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dur = df \\\n",
    "    .withColumn('pickup_date', F.to_date('pickup_datetime')) \\\n",
    "    .withColumn(\"duration_hours\", (F.unix_timestamp(\"dropoff_datetime\") - F.unix_timestamp(\"pickup_datetime\")) / 3600) \\\n",
    "    .groupby(\"pickup_date\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the duration in hours for each trip\n",
    "df = df.withColumn(\"duration_hours\", \n",
    "                   (F.unix_timestamp(\"dropoff_datetime\") - F.unix_timestamp(\"pickup_datetime\")) / 3600)\n",
    "\n",
    "# Find the maximum duration\n",
    "longest_trip_duration = df.agg(F.max(\"duration_hours\")).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631152.5\n"
     ]
    }
   ],
   "source": [
    "print(longest_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_schema = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zone_schema) \\\n",
    "    .csv('../data/taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|     duration_hours|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|              B00271|2019-10-02 00:42:46|2019-10-02 01:25:23|         264|         265|   null|                B00271| 0.7102777777777778|\n",
      "|              B03107|2019-10-02 17:57:32|2019-10-02 19:22:26|          24|         256|   null|                B03107|              1.415|\n",
      "|              B01087|2019-10-01 09:01:20|2019-10-01 09:19:33|         143|         170|   null|                B01087| 0.3036111111111111|\n",
      "|              B02550|2019-10-01 11:21:37|2019-10-01 11:30:56|         264|         167|   null|                B02550|0.15527777777777776|\n",
      "|              B01721|2019-10-01 00:50:10|2019-10-01 00:55:37|         264|         256|   null|                B01721|0.09083333333333334|\n",
      "|              B00310|2019-10-02 20:29:21|2019-10-02 20:38:08|         264|          31|   null|                B02887| 0.1463888888888889|\n",
      "|              B01536|2019-10-01 11:30:07|2019-10-01 11:41:28|         264|         200|   null|                B01536|0.18916666666666668|\n",
      "|              B00756|2019-10-02 06:19:00|2019-10-02 06:52:00|         264|         264|   null|                B00756|               0.55|\n",
      "|              B00536|2019-10-02 11:10:54|2019-10-02 11:20:35|         264|          89|   null|                B00536| 0.1613888888888889|\n",
      "|              B00256|2019-10-01 13:59:37|2019-10-01 14:04:24|         264|         264|   null|                B00256|0.07972222222222222|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artyom\\anaconda3\\envs\\spark_env\\lib\\site-packages\\pyspark\\sql\\dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_zone.registerTempTable(\"zone_lookup\")\n",
    "df.registerTempTable(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|     duration_hours|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|              B00271|2019-10-02 00:42:46|2019-10-02 01:25:23|         264|         265|   null|                B00271| 0.7102777777777778|\n",
      "|              B03107|2019-10-02 17:57:32|2019-10-02 19:22:26|          24|         256|   null|                B03107|              1.415|\n",
      "|              B01087|2019-10-01 09:01:20|2019-10-01 09:19:33|         143|         170|   null|                B01087| 0.3036111111111111|\n",
      "|              B02550|2019-10-01 11:21:37|2019-10-01 11:30:56|         264|         167|   null|                B02550|0.15527777777777776|\n",
      "|              B01721|2019-10-01 00:50:10|2019-10-01 00:55:37|         264|         256|   null|                B01721|0.09083333333333334|\n",
      "|              B00310|2019-10-02 20:29:21|2019-10-02 20:38:08|         264|          31|   null|                B02887| 0.1463888888888889|\n",
      "|              B01536|2019-10-01 11:30:07|2019-10-01 11:41:28|         264|         200|   null|                B01536|0.18916666666666668|\n",
      "|              B00756|2019-10-02 06:19:00|2019-10-02 06:52:00|         264|         264|   null|                B00756|               0.55|\n",
      "|              B00536|2019-10-02 11:10:54|2019-10-02 11:20:35|         264|          89|   null|                B00536| 0.1613888888888889|\n",
      "|              B00256|2019-10-01 13:59:37|2019-10-01 14:04:24|         264|         264|   null|                B00256|0.07972222222222222|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM trips LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_frequent_pickup_query = \"\"\"\n",
    "SELECT \n",
    "    z.Zone, \n",
    "    COUNT(*) as trip_count\n",
    "FROM \n",
    "    trips t\n",
    "JOIN \n",
    "    zone_lookup z \n",
    "ON \n",
    "    t.PULocationID = z.LocationID\n",
    "GROUP BY \n",
    "    z.Zone\n",
    "ORDER BY \n",
    "    trip_count ASC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Zone|trip_count|\n",
      "+--------------------+----------+\n",
      "|         Jamaica Bay|         1|\n",
      "|Governor's Island...|         2|\n",
      "| Green-Wood Cemetery|         5|\n",
      "|       Broad Channel|         8|\n",
      "|     Highbridge Park|        14|\n",
      "|        Battery Park|        15|\n",
      "|Saint Michaels Ce...|        23|\n",
      "|Breezy Point/Fort...|        25|\n",
      "|Marine Park/Floyd...|        26|\n",
      "|        Astoria Park|        29|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "least_frequent_pickup_df = spark.sql(least_frequent_pickup_query)\n",
    "\n",
    "least_frequent_pickup_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
